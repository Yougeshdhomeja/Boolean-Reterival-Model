{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing\n",
    "import pickle\n",
    "import sys\n",
    "#this function remove stop word from text\n",
    "def Remove_Stopwords(text_tokenized):\n",
    "    text_without_sw=[token for token in text_tokenized if token.lower() not in stop_words]\n",
    "    return text_without_sw\n",
    "\n",
    "#this function Remove_Punctuation from text\n",
    "def Remove_Punctuation(text_tokenized_wsw):\n",
    "    text_without_pun=[token for token in text_tokenized_wsw if token not in punctuations]\n",
    "    return text_without_pun\n",
    "\n",
    "#this Function coverts letters into lower.\n",
    "def casefolding(text):\n",
    "    text_casefolded=[]\n",
    "    for token in text:\n",
    "        text_casefolded.append(token.upper())\n",
    "    return text_casefolded\n",
    "\n",
    "\n",
    "#this word tokenize the text into words and replace - with space\n",
    "def word_tokenization(data):\n",
    "    data=data.lower()\n",
    "    characters=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "    special_character=['\\n',' ']\n",
    "    for i in data:\n",
    "        if i == '-':\n",
    "            data=data.replace(i,\" \")\n",
    "        if i not in characters:\n",
    "            if i not in special_character:\n",
    "                data=data.replace(i,\"\")\n",
    "    data=data.split()\n",
    "    return(data)\n",
    "\n",
    "\n",
    "#this function stem the words by porter stemmer\n",
    "def stemming_words(text):\n",
    "    stemmed_words=[]\n",
    "    for token in text:\n",
    "        stemmed_words.append(porter.stem(token))\n",
    "    return stemmed_words\n",
    "#finds the positions of token\n",
    "def find_postions_of_token(text_tokenized, token):\n",
    "    return [position for position, term in enumerate(text_tokenized) if term == token]\n",
    "\n",
    "#check if position exist\n",
    "def check_position_exist(result,positions):\n",
    "    for pos in positions:\n",
    "        if pos  in result:\n",
    "            return 1\n",
    "    return 0\n",
    "#write positional_index to file\n",
    "def Write_Positional_Index_to_file(Positional_index):\n",
    "    sorted_Positional_index = {}\n",
    "    sorted_Positional_index_keys = sorted(Positional_index.keys())  \n",
    "\n",
    "    for term in sorted_Positional_index_keys:\n",
    "        sorted_Positional_index[term] = Positional_index[term]\n",
    "    file = open(\"Positional_index.pkl\", \"wb\")\n",
    "    pickle.dump(sorted_Positional_index, file)\n",
    "    file.close()\n",
    "#check validity of Query\n",
    "def check_validity_of_Query(Query):\n",
    "    flag=0\n",
    "    for i in Query:\n",
    "          if i == '\\\\' :\n",
    "            Query=Query.replace(i,\" \")\n",
    "            flag=1\n",
    "    Query=Query.split()\n",
    "    if len(Query)==3 and flag==1:\n",
    "        if int(Query[2])>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "file=open(\"C:\\\\Users\\\\Yougesh\\\\CS317-IR Dataset for A1\\\\Stopword-List.txt\",'r')\n",
    "\n",
    "stop_words=file.read()\n",
    "\n",
    "punctuations = '''!()-[]{};:'‘\"\\,<>./?@#$%^&*“’”_~'''\n",
    "\n",
    "\n",
    "Positional_Index=dict()\n",
    "\n",
    "list_t=\"\"\n",
    "\n",
    "for x in range(1,51):\n",
    "    \n",
    "    \n",
    "    file=open(\"C:\\\\Users\\\\Yougesh\\\\CS317-IR Dataset for A1\\\\ShortStories\\\\\"+str(x)+\".txt\",'r',encoding=\"utf8\")\n",
    "    \n",
    "    data=file.read()\n",
    "    \n",
    "    text_without_sw=Remove_Stopwords(word_tokenization(data))\n",
    "    \n",
    "    text_cleaned=Remove_Punctuation(text_without_sw)\n",
    "    \n",
    "    stemmed_words=stemming_words(text_cleaned)\n",
    "    \n",
    "    text_casefolded=casefolding(stemmed_words)\n",
    "    #Building positional_index\n",
    "    if x==1:\n",
    "        for token in stemmed_words:\n",
    "            Positional_Index[token]={}\n",
    "            Positional_Index[token][x]=find_postions_of_token(stemmed_words,token)\n",
    "            \n",
    "    else:\n",
    "        list_keys=Positional_Index.keys()\n",
    "        for token in stemmed_words:\n",
    "            if token not in list_keys:\n",
    "                Positional_Index[token]={}\n",
    "        for token in stemmed_words:\n",
    "                if token in list_keys:\n",
    "                    if x not in Positional_Index.get(token):\n",
    "                        Positional_Index[token][x]=find_postions_of_token(stemmed_words,token)\n",
    "Write_Positional_Index_to_file(Positional_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading positional_index file to dictionary object.\n",
    "file = open(\"Positional_index.pkl\", \"rb\")\n",
    "Positional_Index = pickle.load(file)\n",
    "Query=input(\"Enter the Query:\")\n",
    "final_res_doc=[]\n",
    "final_res_set=[]\n",
    "res1=dict()\n",
    "res2=dict()\n",
    "\n",
    "res3=[]\n",
    "\n",
    "res4=[]\n",
    "#check validity of the Query\n",
    "if check_validity_of_Query(Query)==0:\n",
    "    print(\"Invalid Query Syntax\")\n",
    "    sys.exit()\n",
    "\n",
    "for i in Query:\n",
    "          if i == '\\\\' :\n",
    "            Query=Query.replace(i,\" \")\n",
    "Query=Query.split()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "term1=porter.stem(Query[0].lower())\n",
    "term2=porter.stem(Query[1].lower())\n",
    "k=int(Query[2])\n",
    "#if term1 exist in to positional_index\n",
    "if term1 in Positional_Index:\n",
    "    res1=Positional_Index.get(porter.stem(term1))\n",
    "#if term2 exist in to positional_index\n",
    "if term2 in Positional_Index:\n",
    "    res2=Positional_Index.get(porter.stem(term2))\n",
    "\n",
    "\n",
    "positions=[]\n",
    "\n",
    "\n",
    "final_res_doc=[term for term in res1.keys() if term in res2.keys()]\n",
    "\n",
    "for docID in final_res_doc:\n",
    "    res3=res1.get(docID)\n",
    "    res4=res2.get(docID)\n",
    "    for loc in res3:\n",
    "        for i in range(1,k+1):\n",
    "            positions.append(int(loc)+int(i))\n",
    "            \n",
    "            if check_position_exist(res4,positions)==1:\n",
    "                if docID not in final_res_set:\n",
    "                    final_res_set.append(docID)\n",
    "            positions.clear()\n",
    "\n",
    "print(\"Result-Set:\"+str(final_res_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
